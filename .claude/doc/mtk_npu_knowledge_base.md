# MTK NPU çŸ¥è¯†åº“ - å·²çŸ¥é—®é¢˜ä¸æœ€ä½³å®è·µ

æœ¬æ–‡æ¡£è®°å½• MTK NPU æ¨¡å‹ç§»æ¤è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å·²çŸ¥é—®é¢˜ã€è§£å†³æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚

**é‡è¦ï¼šClaude Code åœ¨ç”Ÿæˆä»£ç æ—¶ä¼šè‡ªåŠ¨å‚è€ƒæ­¤æ–‡æ¡£ï¼**

---

## ğŸ“‹ ç›®å½•

1. [å¹³å°é™åˆ¶](#å¹³å°é™åˆ¶)
2. [ä¸æ”¯æŒçš„ç®—å­](#ä¸æ”¯æŒçš„ç®—å­)
3. [Tensor å½¢çŠ¶é™åˆ¶](#tensor-å½¢çŠ¶é™åˆ¶)
4. [å¸¸è§é™·é˜±](#å¸¸è§é™·é˜±)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
6. [å‚è€ƒå®ç°](#å‚è€ƒå®ç°)

---

## ğŸš« å¹³å°é™åˆ¶

### MT8371 ç‰¹å®šé™åˆ¶

#### 1. ä¸æ”¯æŒ 5D Tensor

**é—®é¢˜ï¼š**
```python
# âŒ é”™è¯¯ï¼šMT8371 ä¸æ”¯æŒ 5D tensor
past_key: [num_layers, batch, num_heads, seq_len, head_dim]  # 5D
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# âœ… æ­£ç¡®ï¼šé‡æ–°è®¾è®¡ä¸º 4D tensor
past_key: [num_layers, batch, seq_len, d_model]  # 4D
# å…¶ä¸­ d_model = num_heads * head_dim
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜1)

**é€‚ç”¨åœºæ™¯ï¼š** æ‰€æœ‰ä½¿ç”¨ KV Cache çš„ Transformer æ¨¡å‹

---

## ğŸ”— ç®—å­æ”¯æŒå‚è€ƒ

**å®Œæ•´ç®—å­åˆ—è¡¨**: `/home/xh/projects/MTK_models_zoo/.claude/doc/mtk_mdla_operators.md`

æŸ¥çœ‹å®Œæ•´æ”¯æŒåˆ—è¡¨ï¼š
```bash
cat /home/xh/projects/MTK_models_zoo/.claude/doc/mtk_mdla_operators.md
```

---

## âŒ ä¸æ”¯æŒçš„ç®—å­

### 1. GATHER ç®—å­

**é—®é¢˜ï¼š**
```python
# âŒ Embedding å±‚ä½¿ç”¨ GATHER ç®—å­
embedding = nn.Embedding(vocab_size, d_model)
output = embedding(token_ids)  # GATHER æ“ä½œ
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# âœ… æ–¹æ¡ˆAï¼šå¯¼å‡º embedding weightsï¼ŒCPU ç«¯æŸ¥æ‰¾
# Python ç«¯ï¼š
torch.save(model.embedding.weight, 'embedding_weights.bin')

# C++ ç«¯ï¼š
void embed_tokens(const int64_t* token_ids, int seq_len, float* output) {
    for (int i = 0; i < seq_len; i++) {
        int64_t token_id = token_ids[i];
        const float* src = embedding_weights_.data() + token_id * d_model_;
        memcpy(output + i * d_model_, src, d_model_ * sizeof(float));
    }
}
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜2)

**é€‚ç”¨åœºæ™¯ï¼š** æ‰€æœ‰ NLP æ¨¡å‹ï¼ˆTransformerã€BERTã€GPT ç­‰ï¼‰

---

### 2. masked_fill ç®—å­

**é—®é¢˜ï¼š**
```python
# âŒ ä¸æ”¯æŒ masked_fill
attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# âœ… ä½¿ç”¨åŠ æ³•ä»£æ›¿
# å°† mask ä» 0/1 æ”¹ä¸º 0/-1e9
mask = torch.zeros(seq_len, seq_len)
mask[mask == 0] = -1e9  # æ— æ•ˆä½ç½®

attn_weights = attn_weights + mask  # ç›´æ¥ç›¸åŠ 
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜3)

**é€‚ç”¨åœºæ™¯ï¼š** æ‰€æœ‰ä½¿ç”¨ attention mask çš„æ¨¡å‹

---

### 3. tril ç®—å­

**é—®é¢˜ï¼š**
```python
# âŒ ä¸æ”¯æŒ torch.tril
causal_mask = torch.tril(torch.ones(seq_len, seq_len))
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# âœ… é¢„è®¡ç®— causal maskï¼Œæ³¨å†Œä¸º buffer
def __init__(self):
    # é¢„è®¡ç®— causal mask
    causal_mask = torch.zeros(max_seq_len, max_seq_len)
    for i in range(max_seq_len):
        for j in range(i + 1, max_seq_len):
            causal_mask[i, j] = -1e9

    # æ³¨å†Œä¸º bufferï¼ˆä¼šè¢«åºåˆ—åŒ–ï¼‰
    self.register_buffer('causal_mask', causal_mask)

def forward(self, x):
    seq_len = x.size(1)
    mask = self.causal_mask[:seq_len, :seq_len]
    attn_weights = attn_weights + mask
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜3)

---

## ğŸ“ Tensor å½¢çŠ¶é™åˆ¶

### å›ºå®šå½¢çŠ¶è¦æ±‚

**é—®é¢˜ï¼š**
MTK NPU ç¼–è¯‘éœ€è¦å›ºå®šçš„è¾“å…¥å½¢çŠ¶ï¼Œä¸æ”¯æŒåŠ¨æ€å½¢çŠ¶ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

1. **éŸ³é¢‘æ¨¡å‹ï¼ˆASRï¼‰**
```python
# å›ºå®šéŸ³é¢‘é•¿åº¦
fixed_audio_duration = 10  # ç§’
fixed_frames = 166  # å¯¹åº” 10 ç§’çš„å¸§æ•°

# Padding ç­–ç•¥
if actual_frames < fixed_frames:
    # Pad åˆ°å›ºå®šé•¿åº¦
    padded = F.pad(features, (0, 0, 0, fixed_frames - actual_frames))
else:
    # æˆªæ–­åˆ°å›ºå®šé•¿åº¦
    padded = features[:, :fixed_frames, :]
```

2. **æ–‡æœ¬æ¨¡å‹ï¼ˆNLPï¼‰**
```python
# å›ºå®šåºåˆ—é•¿åº¦
max_seq_len = 64

# Encoder self-attention mask (å¤„ç† padding)
def create_encoder_mask(actual_len, max_len):
    mask = torch.zeros(1, 1, max_len, max_len)
    mask[:, :, :, actual_len:] = -1e9  # Mask padding positions
    return mask
```

**æ¥æºï¼š** SenseVoice å’Œ Helsinki é¡¹ç›®

---

## âš ï¸ å¸¸è§é™·é˜±

### 1. Position Embedding é‡å¤æ·»åŠ 

**é—®é¢˜ï¼š**
```python
# Python æ¨¡å‹å†…éƒ¨å·²æ·»åŠ  position embedding
class MTKEncoder(nn.Module):
    def forward(self, inputs_embeds):
        hidden_states = inputs_embeds + self.embed_positions(...)  # å†…éƒ¨æ·»åŠ 

# C++ ç«¯åˆæ·»åŠ äº†ä¸€æ¬¡
void embed_tokens(...) {
    memcpy(...);
    // âŒ é”™è¯¯ï¼šé‡å¤æ·»åŠ  position embedding
    for (int j = 0; j < d_model_; j++) {
        dst[j] += position_embeddings_[i * d_model_ + j];
    }
}
```

**è§£å†³æ–¹æ¡ˆï¼š**
```cpp
// âœ… C++ ç«¯åªåš token embeddingï¼Œä¸æ·»åŠ  position
void embed_tokens(const int64_t* token_ids, int seq_len, float* output) {
    for (int i = 0; i < seq_len; i++) {
        const float* src = embedding_weights_.data() + token_id * d_model_;
        memcpy(output + i * d_model_, src, d_model_ * sizeof(float));
        // ä¸æ·»åŠ  positionï¼Œæ¨¡å‹å†…éƒ¨ä¼šå¤„ç†
    }
}
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜6)

---

### 2. final_logits_bias ç¼ºå¤±

**é—®é¢˜ï¼š**
HuggingFace çš„ MarianMT ç­‰æ¨¡å‹åœ¨è¾“å‡ºå±‚æœ‰ `final_logits_bias`ï¼Œå®¹æ˜“é—æ¼ã€‚

```python
# âŒ æ¼æ‰ bias
logits = self.lm_head(hidden_states)

# âœ… æ­£ç¡®å®ç°
logits = self.lm_head(hidden_states) + self.final_logits_bias
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
class MTKDecoder(nn.Module):
    def __init__(self):
        # æ·»åŠ  final_logits_bias buffer
        self.register_buffer('final_logits_bias', torch.zeros(1, vocab_size))

    def forward(self, ...):
        logits = self.lm_head(hidden_states) + self.final_logits_bias
        return logits

# åŠ è½½æƒé‡æ—¶å¤åˆ¶
mtk_decoder.final_logits_bias.copy_(hf_model.final_logits_bias)
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜4)

---

### 3. Encoder Padding å¤„ç†

**é—®é¢˜ï¼š**
Encoder å¤„ç† padding tokens æ—¶ï¼Œå¦‚æœä¸åŠ  maskï¼Œpadding ä½ç½®ä¼šå‚ä¸ attention è®¡ç®—ï¼Œå¯¼è‡´è¾“å‡ºé”™è¯¯ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# Python ç«¯
def create_encoder_self_attn_mask(actual_src_len, src_seq_len):
    """
    Shape: [1, 1, src_seq_len, src_seq_len]
    æœ‰æ•ˆä½ç½® = 0, padding ä½ç½® = -1e9
    """
    mask = torch.zeros(1, 1, src_seq_len, src_seq_len)
    mask[:, :, :, actual_src_len:] = -1e9  # padding columns
    return mask

# C++ ç«¯
void create_encoder_self_attn_mask(int actual_src_len, float* output) {
    const float NEG_INF = -1e9f;
    for (int r = 0; r < src_seq_len_; r++) {
        for (int c = 0; c < src_seq_len_; c++) {
            output[r * src_seq_len_ + c] = (c < actual_src_len) ? 0.0f : NEG_INF;
        }
    }
}
```

**æ¥æºï¼š** Helsinki é¡¹ç›® (PORTING_NOTES.md - é—®é¢˜5)

---

## âœ… æœ€ä½³å®è·µ

### 1. æ¨¡å‹ç»“æ„è®¾è®¡åŸåˆ™

**èŒè´£åˆ†ç¦»ï¼š**
```
CPU ç«¯ï¼š
  âœ“ éŸ³é¢‘/å›¾åƒç‰¹å¾æå–ï¼ˆFbank, Mel-spectrogramï¼‰
  âœ“ Tokenization / Embedding lookup
  âœ“ å¤æ‚çš„åå¤„ç†é€»è¾‘ï¼ˆBeam Search, CTC è§£ç ï¼‰
  âœ“ åŠ¨æ€é€»è¾‘ï¼ˆæ¡ä»¶åˆ¤æ–­ã€å¾ªç¯ï¼‰

NPU ç«¯ï¼š
  âœ“ çŸ©é˜µè¿ç®—ï¼ˆLinear, MatMulï¼‰
  âœ“ å·ç§¯æ“ä½œ
  âœ“ Attention æœºåˆ¶
  âœ“ æ¿€æ´»å‡½æ•°ï¼ˆReLU, GELU, Softmaxï¼‰
```

---

### 2. Attention Mask è®¾è®¡

#### Encoder Self-Attention Mask
```python
# Shape: [batch, 1, src_len, src_len]
# ç”¨äºå±è”½ padding positions
mask[:, :, :, actual_len:] = -1e9
```

#### Decoder Self-Attention Mask (with KV Cache)
```python
# Shape: [batch, 1, 1, cache_len + 1]
# å½“å‰ query åªæœ‰ 1 ä¸ª token
mask[:, :, :, :cache_len] = 0      # past cache æœ‰æ•ˆ
mask[:, :, :, cache_len:-1] = -1e9  # æœªä½¿ç”¨çš„ cache ä½ç½®
mask[:, :, :, -1] = 0               # å½“å‰ token æœ‰æ•ˆ
```

#### Decoder Cross-Attention Mask
```python
# Shape: [batch, 1, 1, src_len]
# å±è”½ encoder output çš„ padding
mask[:, :, :, actual_src_len:] = -1e9
```

---

### 3. æ•°å€¼éªŒè¯æµç¨‹

**é€å±‚å¯¹æ¯”ï¼š**
```python
def validate_model(pytorch_model, mtk_model, test_input):
    """é€å±‚å¯¹æ¯”è¾“å‡ºï¼Œç¡®ä¿æ•°å€¼ä¸€è‡´"""

    # 1. å¯¹æ¯” encoder output
    pt_encoder_out = pytorch_model.encoder(test_input)
    mtk_encoder_out = mtk_model.encoder(test_input)
    diff = (pt_encoder_out - mtk_encoder_out).abs().max()
    assert diff < 1e-4, f"Encoder diff: {diff}"

    # 2. å¯¹æ¯” decoder layers
    for i, (pt_layer, mtk_layer) in enumerate(
        zip(pytorch_model.decoder.layers, mtk_model.decoder.layers)
    ):
        pt_out = pt_layer(...)
        mtk_out = mtk_layer(...)
        diff = (pt_out - mtk_out).abs().max()
        assert diff < 1e-4, f"Layer {i} diff: {diff}"

    # 3. å¯¹æ¯” final output
    pt_logits = pytorch_model(test_input)
    mtk_logits = mtk_model(test_input)
    diff = (pt_logits - mtk_logits).abs().max()
    assert diff < 1e-3, f"Final diff: {diff}"
```

---

### 4. C++ ç«¯å®ç°æ£€æŸ¥æ¸…å•

**å¿…é¡»éªŒè¯çš„ç‚¹ï¼š**
- [ ] Embedding æ˜¯å¦æ­£ç¡®ï¼ˆä¸è¦é‡å¤æ·»åŠ  positionï¼‰
- [ ] Attention mask çš„ shape å’Œå€¼æ˜¯å¦æ­£ç¡®
- [ ] KV Cache çš„æ‹¼æ¥é€»è¾‘æ˜¯å¦æ­£ç¡®
- [ ] å†…å­˜æ˜¯å¦æ­£ç¡®é‡Šæ”¾ï¼ˆæ— æ³„æ¼ï¼‰
- [ ] æ•°å€¼æ˜¯å¦ä¸ Python ç«¯ä¸€è‡´ï¼ˆå›ºå®šè¾“å…¥æµ‹è¯•ï¼‰

---

## ğŸ“š å‚è€ƒå®ç°

### SenseVoice (ASR - Encoder Only)

**ä¼˜ç‚¹ï¼š**
- éŸ³é¢‘é¢„å¤„ç†ç®¡é“ï¼ˆkaldi-native-fbankï¼‰
- CTC è§£ç å®ç°
- å›ºå®šé•¿åº¦éŸ³é¢‘å¤„ç†

**å…³é”®æ–‡ä»¶ï¼š**
- `torch_model.py` - è‡ªå®šä¹‰æ¨¡å‹ç»“æ„
- `test_converted_models.py` - éªŒè¯è„šæœ¬
- `sensevoice/sensevoice.cc` - C++ æ¨ç†å®ç°

**é€‚ç”¨åœºæ™¯ï¼š**
- ASR æ¨¡å‹ï¼ˆWhisper Encoder, Wav2Vec2 ç­‰ï¼‰
- Encoder-only æ¶æ„
- éœ€è¦éŸ³é¢‘ç‰¹å¾æå–

---

### Helsinki (Translation - Encoder-Decoder)

**ä¼˜ç‚¹ï¼š**
- 4D KV Cache å®ç°ï¼ˆé¿å… 5D é™åˆ¶ï¼‰
- Encoder-Decoder ååŒ
- Embedding CPU ç«¯å¤„ç†
- å®Œæ•´çš„ Attention Mask è®¾è®¡

**å…³é”®æ–‡ä»¶ï¼š**
- `mtk_model.py` - è‡ªå®šä¹‰æ¨¡å‹ç»“æ„ï¼ˆ4D KV Cacheï¼‰
- `PORTING_NOTES.md` - è¯¦ç»†çš„é—®é¢˜è®°å½•
- `helsinki/helsinki.cc` - C++ æ¨ç†å®ç°

**é€‚ç”¨åœºæ™¯ï¼š**
- ç¿»è¯‘æ¨¡å‹ï¼ˆM2M100, NLLB ç­‰ï¼‰
- Encoder-Decoder æ¶æ„
- éœ€è¦ KV Cache çš„ç”Ÿæˆæ¨¡å‹

---

## ğŸ”„ çŸ¥è¯†åº“æ›´æ–°æµç¨‹

æ¯æ¬¡é‡åˆ°æ–°é—®é¢˜æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼æ·»åŠ ï¼š

```markdown
### X. æ–°é—®é¢˜æ ‡é¢˜

**é—®é¢˜ï¼š**
æè¿°é—®é¢˜ç°è±¡å’Œé”™è¯¯ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆï¼š**
ç»™å‡ºå…·ä½“çš„ä»£ç ç¤ºä¾‹

**æ¥æºï¼š** é¡¹ç›®åç§° (æ—¥æœŸ)

**é€‚ç”¨åœºæ™¯ï¼š** å“ªäº›æ¨¡å‹ä¼šé‡åˆ°è¿™ä¸ªé—®é¢˜
```

---

## ğŸ“ ç‰ˆæœ¬å†å²

- **v1.0** (2026-01-19): åˆå§‹ç‰ˆæœ¬ï¼Œæ•´åˆ Helsinki å’Œ SenseVoice ç»éªŒ
- åç»­æ›´æ–°ï¼šæ¯æ¬¡æ–°é¡¹ç›®å®Œæˆåï¼Œæ·»åŠ æ–°çš„ç»éªŒ

---

**æœ€åæ›´æ–°**: 2026-01-19
**ç»´æŠ¤è€…**: ç®—æ³•å·¥ç¨‹å¸ˆ + Claude Code
